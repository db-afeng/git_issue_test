{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "336ccb05-22f7-498e-9df8-4ba36dc88c2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install langgraph-checkpoint-postgres langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ccbbd07-02ec-4b15-9410-c9d8371eec61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bfa5ccb-c13b-41f5-87b2-39961018fb0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "import os\n",
    "import json\n",
    "from urllib.parse import quote_plus\n",
    "from typing import Any\n",
    "\n",
    "from langgraph.checkpoint.postgres import PostgresSaver\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType\n",
    "\n",
    "client = \"edg\"\n",
    "env_name = \"test\"\n",
    "SCOPE = \"databricks-secrets\"\n",
    "\n",
    "os.environ[\"DATABRICKS_DEFAULT_SERVICE_CREDENTIAL_NAME\"] = (\n",
    "    f\"{env_name}-connector-{client}-data-service\"\n",
    ")\n",
    "\n",
    "# This never changes because lakebase is hosted in prod\n",
    "POSTGRES_HOST = (\n",
    "    \"instance-0b95e886-17ee-4296-9752-6cdf30c0739b.database.azuredatabricks.net\"\n",
    ")\n",
    "\n",
    "\n",
    "def create_pg_url(user: str, password: str, host: str, db: str) -> str:\n",
    "    return (\n",
    "        f\"postgresql://{quote_plus(user)}:{quote_plus(password)}@{host}:5432/{db}\"\n",
    "        \"?sslmode=require\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _to_json(x: Any) -> str:\n",
    "    return json.dumps(x, default=str, ensure_ascii=False)\n",
    "\n",
    "\n",
    "def _split_ai_content(content):\n",
    "    \"\"\"\n",
    "    Returns: (visible_text, thinking_text, has_tool_call_parts)\n",
    "    content may be:\n",
    "      - str\n",
    "      - list of {\"type\": \"...\", ...}\n",
    "    \"\"\"\n",
    "    if content is None:\n",
    "        return (\"\", \"\", False)\n",
    "\n",
    "    if isinstance(content, str):\n",
    "        return (content, \"\", False)\n",
    "\n",
    "    visible = []\n",
    "    thinking = []\n",
    "    has_tool_parts = False\n",
    "\n",
    "    if isinstance(content, list):\n",
    "        for part in content:\n",
    "            if not isinstance(part, dict):\n",
    "                continue\n",
    "            ptype = part.get(\"type\")\n",
    "            if ptype == \"text\":\n",
    "                visible.append(part.get(\"text\", \"\"))\n",
    "            elif ptype == \"reasoning\":\n",
    "                thinking.append(part.get(\"text\") or _to_json(part))\n",
    "            elif ptype in (\"function_call\", \"tool_call\"):\n",
    "                has_tool_parts = True\n",
    "\n",
    "    return (\n",
    "        \"\".join(visible).strip(),\n",
    "        \"\\n\".join([t for t in thinking if t]).strip(),\n",
    "        has_tool_parts,\n",
    "    )\n",
    "\n",
    "\n",
    "def decode_checkpoint_tuple(tup):\n",
    "    \"\"\"Turn one CheckpointTuple into your flattened event rows.\"\"\"\n",
    "    cp = tup.checkpoint or {}\n",
    "    meta = tup.metadata or {}\n",
    "    cfg = (tup.config or {}).get(\"configurable\", {}) or {}\n",
    "\n",
    "    thread_id = cfg.get(\"thread_id\")\n",
    "    checkpoint_id = cp.get(\"id\")\n",
    "    checkpoint_ts = cp.get(\"ts\")\n",
    "    step = meta.get(\"step\")\n",
    "    thread_user = meta.get(\"username\") or cfg.get(\"user_id\") or None\n",
    "\n",
    "    messages = (cp.get(\"channel_values\") or {}).get(\"messages\") or []\n",
    "\n",
    "    events = []\n",
    "    seq = 0\n",
    "\n",
    "    for i, msg in enumerate(messages):\n",
    "        seq += 1\n",
    "        msg_id = (\n",
    "            getattr(msg, \"id\", None)\n",
    "            or f\"{thread_id}:{checkpoint_id}:{i}:{type(msg).__name__}\"\n",
    "        )\n",
    "\n",
    "        visible_text = \"\"\n",
    "        thinking_text = \"\"\n",
    "        tool_name = None\n",
    "        tool_call_id = None\n",
    "        tool_args_json = None\n",
    "\n",
    "        parent_ai_message_id = None\n",
    "\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            event_type = \"human\"\n",
    "            role = \"human\"\n",
    "            name = getattr(msg, \"name\", None) or thread_user\n",
    "            visible_text = (msg.content or \"\").strip()\n",
    "            model = None\n",
    "            input_tokens = output_tokens = total_tokens = None\n",
    "\n",
    "        elif isinstance(msg, AIMessage):\n",
    "            event_type = \"ai\"\n",
    "            role = \"ai\"\n",
    "            name = getattr(msg, \"name\", None) or \"assistant\"\n",
    "\n",
    "            rm = getattr(msg, \"response_metadata\", {}) or {}\n",
    "            um = getattr(msg, \"usage_metadata\", {}) or {}\n",
    "            model = rm.get(\"model_name\") or rm.get(\"model\")\n",
    "\n",
    "            input_tokens = um.get(\"input_tokens\")\n",
    "            output_tokens = um.get(\"output_tokens\")\n",
    "            total_tokens = um.get(\"total_tokens\")\n",
    "\n",
    "            visible_text, thinking_text, _ = _split_ai_content(\n",
    "                getattr(msg, \"content\", None)\n",
    "            )\n",
    "\n",
    "        elif isinstance(msg, ToolMessage):\n",
    "            event_type = \"tool_result\"\n",
    "            role = \"tool\"\n",
    "            name = getattr(msg, \"name\", None)\n",
    "            tool_name = getattr(msg, \"name\", None)\n",
    "            tool_call_id = getattr(msg, \"tool_call_id\", None)\n",
    "            model = None\n",
    "            input_tokens = output_tokens = total_tokens = None\n",
    "\n",
    "        else:\n",
    "            event_type = \"other\"\n",
    "            role = \"other\"\n",
    "            name = getattr(msg, \"name\", None)\n",
    "            model = None\n",
    "            input_tokens = output_tokens = total_tokens = None\n",
    "\n",
    "        events.append(\n",
    "            {\n",
    "                \"thread_id\": thread_id,\n",
    "                \"thread_user\": thread_user,\n",
    "                \"checkpoint_id\": checkpoint_id,\n",
    "                \"checkpoint_ts\": checkpoint_ts,\n",
    "                \"step\": step,\n",
    "                \"seq\": seq,\n",
    "                \"event_type\": event_type,\n",
    "                \"role\": role,\n",
    "                \"message_type\": type(msg).__name__,\n",
    "                \"message_id\": msg_id,\n",
    "                \"name\": name,\n",
    "                \"visible_text\": visible_text,\n",
    "                \"thinking_text\": thinking_text,\n",
    "                \"tool_name\": tool_name,\n",
    "                \"tool_call_id\": tool_call_id,\n",
    "                \"tool_args_json\": tool_args_json,\n",
    "                \"model\": model,\n",
    "                \"input_tokens\": input_tokens,\n",
    "                \"output_tokens\": output_tokens,\n",
    "                \"total_tokens\": total_tokens,\n",
    "                \"content_json\": _to_json(getattr(msg, \"content\", None)),\n",
    "                \"parent_ai_message_id\": parent_ai_message_id,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Add tool_call rows from AIMessage.tool_calls\n",
    "        if isinstance(msg, AIMessage):\n",
    "            for tc in getattr(msg, \"tool_calls\", None) or []:\n",
    "                seq += 1\n",
    "                events.append(\n",
    "                    {\n",
    "                        \"thread_id\": thread_id,\n",
    "                        \"thread_user\": thread_user,\n",
    "                        \"checkpoint_id\": checkpoint_id,\n",
    "                        \"checkpoint_ts\": checkpoint_ts,\n",
    "                        \"step\": step,\n",
    "                        \"seq\": seq,\n",
    "                        \"event_type\": \"tool_call\",\n",
    "                        \"role\": \"tool\",\n",
    "                        \"message_type\": \"ToolCall\",\n",
    "                        \"message_id\": tc.get(\"id\")\n",
    "                        or f\"{thread_id}:{msg_id}:toolcall:{seq}\",\n",
    "                        \"name\": tc.get(\"name\"),\n",
    "                        \"visible_text\": \"\",\n",
    "                        \"thinking_text\": \"\",\n",
    "                        \"tool_name\": tc.get(\"name\"),\n",
    "                        \"tool_call_id\": tc.get(\"id\"),\n",
    "                        \"tool_args_json\": _to_json(tc.get(\"args\")),\n",
    "                        \"model\": model,\n",
    "                        \"input_tokens\": input_tokens,\n",
    "                        \"output_tokens\": output_tokens,\n",
    "                        \"total_tokens\": total_tokens,\n",
    "                        \"content_json\": \"null\",\n",
    "                        \"parent_ai_message_id\": msg_id,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return events\n",
    "\n",
    "\n",
    "def iter_latest_checkpoints_per_thread(saver, limit=None):\n",
    "    \"\"\"Iterate newest checkpoint per thread_id.\"\"\"\n",
    "    seen = set()\n",
    "    for tup in saver.list(None, limit=limit):\n",
    "        cfg = (tup.config or {}).get(\"configurable\", {}) or {}\n",
    "        tid = cfg.get(\"thread_id\")\n",
    "        if not tid or tid in seen:\n",
    "            continue\n",
    "        seen.add(tid)\n",
    "        yield tup\n",
    "\n",
    "\n",
    "def run(\n",
    "    client: str,\n",
    "    env_name: str,\n",
    "    spark: SparkSession | None = None,\n",
    "    *,\n",
    "    limit: int | None = None,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Extract latest checkpoints per thread, flatten them into event rows, and write to:\n",
    "      {ENVIRONMENT_NAME}_{client}.tracking.asa_agent\n",
    "\n",
    "    Returns the fully-qualified destination table name.\n",
    "    \"\"\"\n",
    "    # if not passed, get/create a session (works in jobs + local)\n",
    "    if spark is None:\n",
    "        spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
    "\n",
    "    postgres_user = f\"alex.feng@databricks.com\"\n",
    "    postgres_db = f\"databricks_postgres\"\n",
    "\n",
    "\n",
    "    postgres_pwd = dbutils.secrets.get(scope=\"alex-feng\", key=\"postgres-test\")\n",
    "\n",
    "    postgres_url = create_pg_url(\n",
    "        user=postgres_user,\n",
    "        password=postgres_pwd,\n",
    "        host=POSTGRES_HOST,\n",
    "        db=postgres_db,\n",
    "    )\n",
    "\n",
    "    print(\"Starting New Run\")\n",
    "    print(\"env_name\", env_name, \"client\", client)\n",
    "    print(\"postgres host\", POSTGRES_HOST)\n",
    "    print(\"postgres db\", postgres_db)\n",
    "    print(\"postgres user\", postgres_user)\n",
    "    print(\"limit\", limit)\n",
    "\n",
    "    with PostgresSaver.from_conn_string(postgres_url) as saver:\n",
    "        saver.setup()\n",
    "\n",
    "        events = []\n",
    "        for tup in iter_latest_checkpoints_per_thread(saver, limit=limit):\n",
    "            print(\"*** processing new conversation thread ***\")\n",
    "            events.extend(decode_checkpoint_tuple(tup))\n",
    "\n",
    "    print(\"Completed PostgresSaver\")\n",
    "\n",
    "    cols = [\n",
    "        \"thread_id\",\n",
    "        \"thread_user\",\n",
    "        \"checkpoint_id\",\n",
    "        \"checkpoint_ts\",\n",
    "        \"step\",\n",
    "        \"seq\",\n",
    "        \"event_type\",\n",
    "        \"role\",\n",
    "        \"message_type\",\n",
    "        \"message_id\",\n",
    "        \"name\",\n",
    "        \"visible_text\",\n",
    "        \"thinking_text\",\n",
    "        \"tool_name\",\n",
    "        \"tool_call_id\",\n",
    "        \"tool_args_json\",\n",
    "        \"model\",\n",
    "        \"input_tokens\",\n",
    "        \"output_tokens\",\n",
    "        \"total_tokens\",\n",
    "        \"content_json\",\n",
    "        \"parent_ai_message_id\",\n",
    "    ]\n",
    "\n",
    "    schema = StructType(\n",
    "        [\n",
    "            StructField(\"thread_id\", StringType(), True),\n",
    "            StructField(\"thread_user\", StringType(), True),\n",
    "            StructField(\"checkpoint_id\", StringType(), True),\n",
    "            StructField(\"checkpoint_ts\", StringType(), True),\n",
    "            StructField(\"step\", IntegerType(), True),\n",
    "            StructField(\"seq\", IntegerType(), True),\n",
    "            StructField(\"event_type\", StringType(), True),\n",
    "            StructField(\"role\", StringType(), True),\n",
    "            StructField(\"message_type\", StringType(), True),\n",
    "            StructField(\"message_id\", StringType(), True),\n",
    "            StructField(\"name\", StringType(), True),\n",
    "            StructField(\"visible_text\", StringType(), True),\n",
    "            StructField(\"thinking_text\", StringType(), True),\n",
    "            StructField(\"tool_name\", StringType(), True),\n",
    "            StructField(\"tool_call_id\", StringType(), True),\n",
    "            StructField(\"tool_args_json\", StringType(), True),\n",
    "            StructField(\"model\", StringType(), True),\n",
    "            StructField(\"input_tokens\", LongType(), True),\n",
    "            StructField(\"output_tokens\", LongType(), True),\n",
    "            StructField(\"total_tokens\", LongType(), True),\n",
    "            StructField(\"content_json\", StringType(), True),\n",
    "            StructField(\"parent_ai_message_id\", StringType(), True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    sdf = spark.createDataFrame(events, schema=schema).select(*cols)\n",
    "\n",
    "    dest_catalog = \"alex_feng\"\n",
    "    dest_table = \"alex_feng.goodwork_test.dest_table\"\n",
    "\n",
    "    print(\"Writing Table Now\")\n",
    "    (\n",
    "        sdf.write.format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"overwriteSchema\", \"true\")\n",
    "        .saveAsTable(dest_table)\n",
    "    )\n",
    "\n",
    "    return dest_table\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Starting Environment {env_name} (client={client})\")\n",
    "    run(client=client, env_name=env_name, spark=spark)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e404e9a5-73b4-4103-859d-438268117414",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "scanned = 0\n",
    "unique = 0\n",
    "seen = set()\n",
    "\n",
    "\n",
    "with PostgresSaver.from_conn_string(postgres_url) as saver:\n",
    "    saver.setup()\n",
    "\n",
    "    for tup in saver.list(None, limit=None):\n",
    "        scanned += 1\n",
    "        cfg = (tup.config or {}).get(\"configurable\", {}) or {}\n",
    "        tid = cfg.get(\"thread_id\")\n",
    "\n",
    "        if tid and tid not in seen:\n",
    "            seen.add(tid)\n",
    "            unique += 1\n",
    "\n",
    "        if scanned % 5000 == 0:\n",
    "            print(\n",
    "                \"scanned\",\n",
    "                scanned,\n",
    "                \"unique_threads\",\n",
    "                unique,\n",
    "                \"elapsed_s\",\n",
    "                round(time.time() - t0, 1),\n",
    "            )\n",
    "\n",
    "print(\n",
    "    \"FINAL scanned\",\n",
    "    scanned,\n",
    "    \"unique_threads\",\n",
    "    unique,\n",
    "    \"elapsed_s\",\n",
    "    round(time.time() - t0, 1),\n",
    ")\n",
    " "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Untitled Notebook 2026-02-05 10:43:53",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}